{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "  \n",
      "Punctuation test! I don't think that the symbols are being correctly removed. so let's make a test with alot of word's with weird symbols in them. Hello world! \n",
      "\n",
      "Words: ['Punctuation', 'test!', 'I', \"don't\", 'think', 'that', 'the', 'symbols', 'are', 'being', 'correctly', 'removed.', 'so', \"let's\", 'make', 'a', 'test', 'with', 'alot', 'of', \"word's\", 'with', 'weird', 'symbols', 'in', 'them.', 'Hello', 'world!']\n",
      "Sentences: [' \\nPunctuation test!', \"I don't think that the symbols are being correctly removed.\", \"so let's make a test with alot of word's with weird symbols in them.\", 'Hello world!']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Importing Required Libraries\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "# Set NLTK data path and download punkt once\n",
    "\n",
    "\n",
    "#This one right here. make sure to change this to a local file, idk why the download directy causes a lookup error. ->>>>\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "nltk_data_dir = r\"C:\\Users\\Collin\\Documents\\School\\CSEverything\\CS584\\nltk_data\"\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "nltk.data.path.insert(0, nltk_data_dir)\n",
    "nltk.download('punkt', download_dir=nltk_data_dir, quiet=True)\n",
    "\n",
    "# Sample Text \n",
    "text = \"\"\" \n",
    "Punctuation test! I don't think that the symbols are being correctly removed. so let's make a test with alot of word's with weird symbols in them. Hello world! \n",
    "\"\"\"\n",
    "\n",
    "# Print original text\n",
    "print(\"Original Text:\\n\", text)\n",
    "\n",
    "# Create basic tokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Tokenization\n",
    "words = text.split()  # Simple word tokenization\n",
    "sentences = tokenizer.tokenize(text)  # Sentence tokenization using PunktSentenceTokenizer\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization ‚úÇÔ∏è\n",
    "Tokenization is the process of splitting text into individual words or sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal üö´\n",
    "Stopwords are common words that may not contribute much meaning, such as \"the,\" \"is,\" \"in,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without Punctuation: ['punctuation', 'test!', 'think', 'symbol', 'correctly', 'removed.', \"let's\", 'make', 'test', 'alot', \"word's\", 'weird', 'symbol', 'them.', 'hello', 'world!']\n",
      "Filtered Words: ['Punctuation', 'test!', 'think', 'symbols', 'correctly', 'removed.', \"let's\", 'make', 'test', 'alot', \"word's\", 'weird', 'symbols', 'them.', 'Hello', 'world!']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation Removal\n",
    "no_punctuation = [word for word in lowercased_words if word not in string.punctuation]\n",
    "\n",
    "print(\"Words without Punctuation:\", no_punctuation)# Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming üå±\n",
    "Stemming reduces words to their base or root form. It may not always produce actual words, but it helps in reducing inflected words to a common base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['punctuat', 'test!', 'think', 'symbol', 'correctli', 'removed.', \"let'\", 'make', 'test', 'alot', \"word'\", 'weird', 'symbol', 'them.', 'hello', 'world!']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization üå±\n",
    "Lemmatization also reduces words to their base form, but it aims to return actual words that belong to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['Punctuation', 'test!', 'think', 'symbol', 'correctly', 'removed.', \"let's\", 'make', 'test', 'alot', \"word's\", 'weird', 'symbol', 'them.', 'Hello', 'world!']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lowercasing üî°\n",
    "Lowercasing helps in maintaining consistency, especially for tasks where case sensitivity does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Words: ['punctuation', 'test!', 'think', 'symbol', 'correctly', 'removed.', \"let's\", 'make', 'test', 'alot', \"word's\", 'weird', 'symbol', 'them.', 'hello', 'world!']\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing\n",
    "lowercased_words = [word.lower() for word in lemmatized_words]\n",
    "\n",
    "print(\"Lowercased Words:\", lowercased_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Punctuation Removal ‚ùå\n",
    "Punctuation removal helps in reducing the vocabulary size and noise in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Punctuation', 'test', 'I', 'dont', 'think', 'that', 'the', 'symbols', 'are', 'being', 'correctly', 'removed', 'so', 'lets', 'make', 'a', 'test', 'with', 'alot', 'of', 'words', 'with', 'weird', 'symbols', 'in', 'them', 'Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation from each word\n",
    "clean_words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "\n",
    "print(clean_words)  # Output: ['Hello', 'world']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Normalization üìù\n",
    "Text normalization is the process of transforming text into a single canonical form. It involves combining all the above steps to clean text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: I have <NUM> apples and <NUM> bananas.\n"
     ]
    }
   ],
   "source": [
    "# Text Normalization Example\n",
    "def normalize_text(text):\n",
    "    # Replace numbers with a placeholder\n",
    "    normalized_text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    # Handle any other custom normalization rules here\n",
    "    return normalized_text\n",
    "\n",
    "normalized_text = normalize_text('I have 3 apples and 20 bananas.')\n",
    "print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Preprocessing Pipeline\n",
    "Now that we have explored each preprocessing step individually, let's combine them into a complete preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: ['Punctuation', 'test', 'I', 'dont', 'think', 'that', 'the', 'symbols', 'are', 'being', 'correctly', 'removed', 'so', 'lets', 'make', 'a', 'test', 'with', 'alot', 'of', 'words', 'with', 'weird', 'symbols', 'in', 'them', 'Hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization (using the method we know works)\n",
    "    words = text.split()  # Simple word tokenization like before\n",
    "\n",
    "    # Stopword Removal\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "\n",
    "    # Lowercasing\n",
    "    lowercased_words = [word.lower() for word in lemmatized_words]\n",
    "\n",
    "    # Remove punctuation from each word\n",
    "    clean_words = [re.sub(r'[^\\w\\s]', '', word) for word in words]\n",
    "\n",
    "    # Return the processed text\n",
    "    return clean_words\n",
    "\n",
    "# Run the preprocessing pipeline on the sample text\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we explored several essential text preprocessing techniques that form the backbone of many NLP tasks. These steps help transform raw text into a format suitable for various natural language processing applications, such as sentiment analysis, text classification, and more.\n",
    "\n",
    "Experiment with these techniques and apply them to your own text datasets to see the power of text preprocessing in action! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
