{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/omar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/omar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/omar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/omar/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      " \n",
      "Hello, world! Welcome to text preprocessing. Let's explore how to clean and prepare text data for NLP tasks.\n",
      "In this notebook, we'll cover various techniques like tokenization, stopword removal, stemming, lemmatization, \n",
      "lowercasing, punctuation removal, and text normalization. Let's get started! üöÄ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample Text\n",
    "text = \"\"\"\n",
    "Hello, world! Welcome to text preprocessing. Let's explore how to clean and prepare text data for NLP tasks.\n",
    "In this notebook, we'll cover various techniques like tokenization, stopword removal, stemming, lemmatization, \n",
    "lowercasing, punctuation removal, and text normalization. Let's get started! üöÄ\n",
    "\"\"\"\n",
    "# Display the Original Text\n",
    "print(\"Original Text:\\n\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization ‚úÇÔ∏è\n",
    "Tokenization is the process of splitting text into individual words or sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Hello', ',', 'world', '!', 'Welcome', 'to', 'text', 'preprocessing', '.', 'Let', \"'s\", 'explore', 'how', 'to', 'clean', 'and', 'prepare', 'text', 'data', 'for', 'NLP', 'tasks', '.', 'In', 'this', 'notebook', ',', 'we', \"'ll\", 'cover', 'various', 'techniques', 'like', 'tokenization', ',', 'stopword', 'removal', ',', 'stemming', ',', 'lemmatization', ',', 'lowercasing', ',', 'punctuation', 'removal', ',', 'and', 'text', 'normalization', '.', 'Let', \"'s\", 'get', 'started', '!', 'üöÄ']\n",
      "Sentences: ['\\nHello, world!', 'Welcome to text preprocessing.', \"Let's explore how to clean and prepare text data for NLP tasks.\", \"In this notebook, we'll cover various techniques like tokenization, stopword removal, stemming, lemmatization, \\nlowercasing, punctuation removal, and text normalization.\", \"Let's get started!\", 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "words = word_tokenize(text) # Word Tokenization\n",
    "sentences = sent_tokenize(text) # Sentence Tokenization\n",
    "\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stopword Removal üö´\n",
    "Stopwords are common words that may not contribute much meaning, such as \"the,\" \"is,\" \"in,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Words: ['Hello', ',', 'world', '!', 'Welcome', 'text', 'preprocessing', '.', 'Let', \"'s\", 'explore', 'clean', 'prepare', 'text', 'data', 'NLP', 'tasks', '.', 'notebook', ',', \"'ll\", 'cover', 'various', 'techniques', 'like', 'tokenization', ',', 'stopword', 'removal', ',', 'stemming', ',', 'lemmatization', ',', 'lowercasing', ',', 'punctuation', 'removal', ',', 'text', 'normalization', '.', 'Let', \"'s\", 'get', 'started', '!', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "# Stopword Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming üå±\n",
    "Stemming reduces words to their base or root form. It may not always produce actual words, but it helps in reducing inflected words to a common base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['hello', ',', 'world', '!', 'welcom', 'text', 'preprocess', '.', 'let', \"'s\", 'explor', 'clean', 'prepar', 'text', 'data', 'nlp', 'task', '.', 'notebook', ',', \"'ll\", 'cover', 'variou', 'techniqu', 'like', 'token', ',', 'stopword', 'remov', ',', 'stem', ',', 'lemmat', ',', 'lowercas', ',', 'punctuat', 'remov', ',', 'text', 'normal', '.', 'let', \"'s\", 'get', 'start', '!', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization üå±\n",
    "Lemmatization also reduces words to their base form, but it aims to return actual words that belong to the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['Hello', ',', 'world', '!', 'Welcome', 'text', 'preprocessing', '.', 'Let', \"'s\", 'explore', 'clean', 'prepare', 'text', 'data', 'NLP', 'task', '.', 'notebook', ',', \"'ll\", 'cover', 'various', 'technique', 'like', 'tokenization', ',', 'stopword', 'removal', ',', 'stemming', ',', 'lemmatization', ',', 'lowercasing', ',', 'punctuation', 'removal', ',', 'text', 'normalization', '.', 'Let', \"'s\", 'get', 'started', '!', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lowercasing üî°\n",
    "Lowercasing helps in maintaining consistency, especially for tasks where case sensitivity does not matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased Words: ['hello', ',', 'world', '!', 'welcome', 'text', 'preprocessing', '.', 'let', \"'s\", 'explore', 'clean', 'prepare', 'text', 'data', 'nlp', 'task', '.', 'notebook', ',', \"'ll\", 'cover', 'various', 'technique', 'like', 'tokenization', ',', 'stopword', 'removal', ',', 'stemming', ',', 'lemmatization', ',', 'lowercasing', ',', 'punctuation', 'removal', ',', 'text', 'normalization', '.', 'let', \"'s\", 'get', 'started', '!', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing\n",
    "lowercased_words = [word.lower() for word in lemmatized_words]\n",
    "\n",
    "print(\"Lowercased Words:\", lowercased_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Punctuation Removal ‚ùå\n",
    "Punctuation removal helps in reducing the vocabulary size and noise in text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without Punctuation: ['hello', 'world', 'welcome', 'text', 'preprocessing', 'let', \"'s\", 'explore', 'clean', 'prepare', 'text', 'data', 'nlp', 'task', 'notebook', \"'ll\", 'cover', 'various', 'technique', 'like', 'tokenization', 'stopword', 'removal', 'stemming', 'lemmatization', 'lowercasing', 'punctuation', 'removal', 'text', 'normalization', 'let', \"'s\", 'get', 'started', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation Removal\n",
    "no_punctuation = [word for word in lowercased_words if word not in string.punctuation]\n",
    "\n",
    "print(\"Words without Punctuation:\", no_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Normalization üìù\n",
    "Text normalization is the process of transforming text into a single canonical form. It involves combining all the above steps to clean text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: I have <NUM> apples and <NUM> bananas.\n"
     ]
    }
   ],
   "source": [
    "# Text Normalization Example\n",
    "def normalize_text(text):\n",
    "    # Replace numbers with a placeholder\n",
    "    normalized_text = re.sub(r'\\d+', '<NUM>', text)\n",
    "    # Handle any other custom normalization rules here\n",
    "    return normalized_text\n",
    "\n",
    "normalized_text = normalize_text('I have 3 apples and 20 bananas.')\n",
    "print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Preprocessing Pipeline\n",
    "Now that we have explored each preprocessing step individually, let's combine them into a complete preprocessing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Text: ['hello', 'world', 'welcom', 'text', 'preprocess', 'let', \"'s\", 'explor', 'clean', 'prepar', 'text', 'data', 'nlp', 'task', 'notebook', \"'ll\", 'cover', 'variou', 'techniqu', 'like', 'token', 'stopword', 'remov', 'stem', 'lemmat', 'lowercas', 'punctuat', 'remov', 'text', 'normal', 'let', \"'s\", 'get', 'start', 'üöÄ']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Stopword Removal\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in stemmed_words]\n",
    "\n",
    "    # Lowercasing\n",
    "    lowercased_words = [word.lower() for word in lemmatized_words]\n",
    "\n",
    "    # Punctuation Removal\n",
    "    no_punctuation = [word for word in lowercased_words if word not in string.punctuation]\n",
    "\n",
    "    # Return the processed text\n",
    "    return no_punctuation\n",
    "\n",
    "# Run the preprocessing pipeline on the sample text\n",
    "processed_text = preprocess_text(text)\n",
    "\n",
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we explored several essential text preprocessing techniques that form the backbone of many NLP tasks. These steps help transform raw text into a format suitable for various natural language processing applications, such as sentiment analysis, text classification, and more.\n",
    "\n",
    "Experiment with these techniques and apply them to your own text datasets to see the power of text preprocessing in action! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
