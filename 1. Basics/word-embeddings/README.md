# Word Embeddings ðŸ“š

Word embeddings are a type of word representation that allows words to be represented as vectors in a continuous vector space. These vectors are learned in such a way that words that have similar meanings will have similar representations. Word embeddings have been shown to capture semantic relationships between words, and are used in many natural language processing applications.

## Key Techniques ðŸ”§

- **Word2Vec** ðŸ“š: Learn word embeddings using a shallow neural network model.
- **GloVe** ðŸ§¤: Pre-trained word vectors trained on large corpora for various languages.
- **FastText** âš¡: Library for learning word representations and sentence classification.
- **BERT Embeddings** ðŸ¤–: Bidirectional Encoder Representations from Transformers for word embeddings.

This notebook explores the **Word2Vec** and **GloVe** word embeddings, and demonstrates how to use them for various NLP tasks.

**Let's dive into the notebook** ðŸ“”
